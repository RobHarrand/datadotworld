{
    "collab_server" : "",
    "contents" : "# Load libraries and key data ---------------------------------------------\n\nlibrary(twitteR)\nlibrary(ROAuth)\nlibrary(httr)\nlibrary(streamR)\nlibrary(translateR)\nlibrary('wordcloud')\nlibrary('tm')\n\nsetwd('/home/rob/Dropbox/Kaggle/JihadWatch')\n\n# STREAMING\n\n# The following four lines assign the right values to the variables that\n# are needed for the API call.\n\nrequestURL <- \"https://api.twitter.com/oauth/request_token\"\naccessURL <- \"https://api.twitter.com/oauth/access_token\"\nauthURL <- \"https://api.twitter.com/oauth/authorize\"\n\n# The string within the quotation marks has to be replaced with the actual\n# consumerKey and consumerSecret.\nconsumerKey <- \"UNebmewdL7fjHIyVPkuQGjVOO\"\nconsumerSecret <- \"XRvonr6F4VjlAwY0AOmKAdP1zqdx79IDkbZAl2xE71YvLOgCrp\"\n\n# Set API Keys\napi_key <- \"UNebmewdL7fjHIyVPkuQGjVOO\"\napi_secret <- \"XRvonr6F4VjlAwY0AOmKAdP1zqdx79IDkbZAl2xE71YvLOgCrp\"\naccess_token <- \"868190366805229568-w9npyLxKg0jqi4IoZcU8lFUbDYY3JL6\"\naccess_token_secret <- \"EyNTquy0NfUK8V5lr0CZXKpN8qIJPQ9STXnaCaFkDf8K7\"\nmy_oauth = setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n\n# The next two lines establish a connection to the Twitter API.\n# The system will print a URL which should be copied in a browser to receive a PIN number.\n# This PIN has to be entered in the R-console.\nmy_oauth <- OAuthFactory$new(consumerKey = consumerKey, \n                             consumerSecret = consumerSecret, \n                             requestURL = requestURL, \n                             accessURL = accessURL, \n                             authURL = authURL)\n\nmy_oauth$handshake(cainfo = system.file(\"CurlSSL\", \"cacert.pem\", package = \"RCurl\"))\n# Once the connection is established we can save it so that we do not have\n# repeat this process.\n\n\n\n# Load/save data ----------------------------------------------------------\n\nsave(my_oauth, file = \"my_oauth.Rdata\") #Authenication data\nload(file = \"my_oauth.Rdata\") #Authenication data\n\n#setwd('c:\\\\r')\n#save(tweets.all, file = 'tweets.RData')\nload(file = 'tweets.RData') #LOAD THIS AS IT'S THE TWEET TEMPLATE\n\n# noTs_history = data.frame(Date=as.Date(character()),\n#                           noTs=character(),\n#                           noRTs=character(),\n#                           stringsAsFactors=FALSE)\n\n#write.csv(noTs_history, file = 'noTs.csv')\nnoTs_history = read.csv('noTs.csv')\n\n\n# Scraping setup and code - 'Good Morning' -------------------------------------------------\n\ni = 1\nSys.time()\n\n#while (i <= 280) {\nwhile (i <= 10000) {\n  \nfilterStream(\"tw_t.json\", \n             timeout = 3600, \n             oauth = my_oauth, \n             track = c('taghut', \n                       'mushrikeen', \n                       'rafidah', \n                       'kufar', \n                       'kuffir',\n                       'kuffar', \n                       'jizya', \n                       'khilafa'), \n             language = 'en')\n#filterStream(\"tw_t.json\", timeout = 30, oauth = my_oauth, track = c('dog', 'cat'), language = 'en')\n#filterStream(\"tw_t.json\", timeout = 10, oauth = my_oauth, track = 'hello', language = 'en')\ntweets_t = parseTweets(\"tw_t.json\")\n\n#ex = tweets_gm$lang != 'ar'\n#tweets_gm = tweets_gm[!ex,]\n    \n#ex = grepl('RT', tweets_gm$text, ignore.case = FALSE) #Remove the RTs\n#tweets_gm = tweets_gm[!ex,]\n\n#ex = grepl('taghut|mushrikeen|rafidah|kufar', tweets_gm$text, ignore.case = TRUE) #Remove anything without good morning in the main tweet text\n#tweets_gm = tweets_gm[ex,]\n\n#ex = is.na(tweets_gm$place_lat) #Remove any with missing place_latitude information\n#tweets_gm = tweets_gm[!ex,]\n\n#tweets.all = rbind(tweets.all, tweets_t) #Add to the collection\n\n\n# \n# res <- translate(content.vec = \"Hello world\", \n#                  microsoft.client.id = \"dc10d31f-4c1f-4e27-8928-8bc85bfd0e6d\", \n#                  microsoft.client.secret = \"790dc39f94ba40ea86cc11eb71aa2f2a\", \n#                  source.lang = \"en\", \n#                  target.lang = \"de\")\n# res\n\n\n\nremoveURL <- function(x) gsub(\"http[[:alnum:][:punct:]]*\", \"\", x)\ntweets_t$text = removeURL(tweets_t$text)\n\nremoveStuff <- function(x) gsub(\"[[:punct:]]\", \" \", x)\ntweets_t$text = removeStuff(tweets_t$text)\n\nnoTs = length(tweets_t$text)\nnoRTs = table(grepl('RT', tweets_t$text, ignore.case = FALSE))[2]\n\ntweets_t$text <- iconv(tweets_t$text, 'UTF-8', 'ASCII')\n\ncorpus = Corpus(VectorSource(list(tweets_t$text)))\ncorpus = tm_map(corpus, removePunctuation)\ncorpus = tm_map(corpus, content_transformer(tolower))\ncorpus = tm_map(corpus, removeNumbers) \ncorpus = tm_map(corpus, stripWhitespace)\n#corpus = tm_map(corpus, function(x) iconv(enc2utf8(x), sub = \"byte\"))\ncorpus = tm_map(corpus, removeWords, stopwords('english'))\n\ndtm_tweets = DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))\nfreq_tweets <- colSums(as.matrix(dtm_tweets))\n\n#sentiments = calculate_sentiment(names(freq_tweets))\n#sentiments = cbind(sentiments, as.data.frame(freq_tweets))\n\nfreq = as.data.frame(freq_tweets)\n\nkeywords = c('taghut', 'mushrikeen', 'rafidah', 'kufar', 'kuffir', 'kuffar', 'jizya', 'khilafa')\n\nfreq$Words = row.names(freq)\nex = freq$Words %in% keywords\nfreq = freq[!ex,]\n\nnoTs_new = data.frame('Date' = Sys.time(), 'noTs' = noTs, 'noRTs' = noRTs)\nnoTs_history = rbind(noTs_history, noTs_new)\n\ndate_for_file = gsub(' ', '_', noTs_new$Date)\ndate_for_file = gsub(':', '_', date_for_file)\n\n\npng(paste('t', res = 800, width = 500, height = 1000, date_for_file, '.png', sep = \"\"))\n\n\npar(mfrow=c(2,1))\npar(mar=c(4,4,1,4))\n\nset.seed(100)\nwordcloud(row.names(freq),\n          freq$freq_tweets, \n          min.freq=max(freq$freq_tweets)*0.3,\n          colors=brewer.pal(6,\"Dark2\"),\n          random.order = T,\n          random.color = T,\n          rot.per = 0.3,\n          scale = c(1.5, 0.1))\n\nplot(as.POSIXct(noTs_history$Date), \n     noTs_history$noTs, \n     xlim = c(as.POSIXct(noTs_history$Date[1], format=\"%Y-%m-%d %H:%M:%S\"), \n              as.POSIXct(noTs_history$Date[length(noTs_history$Date)], format=\"%Y-%m-%d %H:%M:%S\")),\n     ylim = c(0, max(noTs_history$noTs)*1.2),\n     ylab = 'No. of tweets',\n     xlab = 'Date',\n     type = 'l',\n     lwd = 2)\npoints(as.POSIXct(noTs_history$Date), \n       noTs_history$noTs,\n       pch = 16)\nlines(as.POSIXct(noTs_history$Date), \n       noTs_history$noRTs,\n       col = 'red',\n      lwd = 2)\npoints(as.POSIXct(noTs_history$Date), \n       noTs_history$noRTs,\n       pch = 16,\n       col = 'red')\nlegend(as.POSIXct(noTs_history$Date[1], format=\"%Y-%m-%d %H:%M:%S\")+100,\n       max(noTs_history$noTs)*1.1,\n       lty=c(1,1),\n       lwd=c(2.5,2.5),\n       legend =c('Tweets', 'Re-tweets'),\n       col=c('black', 'red'))\n\ndev.off()\n\ni=i+1\n\ntweet(paste(\"Update at\", Sys.time(), '#radicalism'), \n      mediaPath = paste('t', date_for_file, '.png', sep = \"\"))\n\n\nfn <- \"tw_t.json\"\nif (file.exists(fn)) file.remove(fn)\n\n\n#tweet('test')\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n# Scraping setup and code - 'dog vets' ---------------------------------\n\n\ni = 1\n\n#while (i <= 280) {\n  \nwhile (i <= 1) {\n  \n  filterStream(\"tw_dog.json\", timeout = 300, oauth = my_oauth, track = 'dog, vets', language = 'en')\n  tweets_dog = parseTweets(\"tw_dog.json\")\n  \n  ex = grepl('RT', tweets_dog$text, ignore.case = FALSE) #Remove the RTs\n  tweets_dog = tweets_dog[!ex,]\n  \n  #ex = grepl('dog & vets', tweets_dog$text, ignore.case = TRUE) #Remove anything without 'walk the dog' in the main tweet text\n  #tweets_dog = tweets_dog[ex,]\n  \n  #ex = is.na(tweets_dog$place_lat) #Remove any with missing place_latitude information\n  #tweets_dog = tweets_dog[!ex,]\n  \n  tweets.all = rbind(tweets.all, tweets_dog) #Add to the collection\n  \n  dup = duplicated(tweets.all$text)\n  tweets.all = tweets.all[!dup,]\n  \n  i=i+1\n  \n  Sys.sleep(5)\n  \n}\n\n\n#write.csv(tweets.all, 'vet_tweets_raw.csv')\n#tweets.all = read.csv('vet_tweets_raw.csv')\n\n#dup = duplicated(tweets.all$text)\n#tweets.unique = tweets.all[!dup,]\n\n#write.csv(tweets.unique, 'vet_tweets_unique.csv')\n\ntweets.unique = read.csv('vet_tweets_unique.csv', stringsAsFactors = F)\n\nplot(tweets.unique$place_lat, tweets.unique$place_lon)\n\n\n\n\n#Load libraries,\nrequire(maptools)\n\n#Get a world map,\ndata(wrld_simpl)\n\npar(mar = c(0,0,1,0),\n    pin = c(4,2),\n    pty = \"m\",\n    xaxs = \"i\",\n    xaxt = 'n',\n    xpd = FALSE,\n    yaxs = \"i\",\n    yaxt = 'n')\n\n\n  # plot(wrld_simpl, \n  #      col='dark green', \n  #      bg='white', \n  #      border='black', \n  #      ann=FALSE, \n  #      axes = FALSE, \n  #      main = \"Walk my dog\", \n  #      xlim = c(-120,10), \n  #      ylim = c(25,53))\n  \n  plot(wrld_simpl, \n       col='dark green', \n       bg='white', \n       border='black', \n       ann=FALSE, \n       axes = FALSE, \n       main = \"Vets!\")\n  \n  points(tweets.unique$place_lon, \n         tweets.unique$place_lat, \n         pch = 16, \n         cex = 0.5, \n         col = 'red')\n  \n  # text_temp = tweets.unique$text[1]\n  # text_temp\n  # \n  # text(x=tweets.unique$place_lon[1]-5, \n  #      y=tweets.unique$place_lat[1]-6, \n  #      labels=tweets.unique$text[1],\n  #      col = 'red',\n  #      cex = 0.5)\n  # \n  # arrows(x0=tweets.unique$place_lon[1]-5, \n  #        y0=tweets.unique$place_lat[1]-5, \n  #        x1=tweets.unique$place_lon[1], \n  #        y1=tweets.unique$place_lat[1], \n  #        col='blue', \n  #        length=0.1, \n  #        lwd=3)\n  \ndev.off()\n  \n\n# Cleaning and plotting ---------------------------------------------------\n\n#load(file = 'tweets_all.RData')\n\n#ex = duplicated(tweets.all$id_str)\n#tweets.all = tweets.all[!ex,]\n\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ntweets.unique$Date = sapply(strsplit(tweets.unique$created_at, \" \"), \"[[\", 3)\ntweets.unique$Date = paste(tweets.unique$Date,\"-12-16\", sep = \"\")\ntweets.unique$Time = sapply(strsplit(tweets.unique$created_at, \" \"), \"[[\", 4)\ntweets.unique$DateTime = paste(tweets.unique$Date, tweets.unique$Time)\ntweets.unique$DateTime = as.POSIXct(tweets.unique$DateTime, format = \"%d-%m-%y %H:%M:%S\")\ntweets.unique$Time = as.POSIXct(tweets.unique$Time, format = \"%H:%M:%S\")\n\ntweets.unique.arranged = arrange(tweets.unique, DateTime)\n\nplot(tweets.unique.arranged$DateTime)\nhist(tweets.unique.arranged$DateTime, breaks = 20)\n\n\n\n#tweets.unique$Time = as.POSIXlt(tweets.unique$Time, format = \"%H:%M:%S\")\n\ntweets.unique.arranged$Hour = hour(tweets.unique.arranged$Time)\ntweets.unique.arranged$Minute = minute(tweets.unique.arranged$Time)\n\n#qplot(seq(1,length(tweets.unique$text),1), tweets.unique$DateTime)\n\n\n\n# i=0\n# j=0\n# k=0\n# \n# while (i<=23) {\n# \n#     while (j<=59) {\n#     \n#         tweets.unique.arranged$Time_bin[tweets.unique.arranged$Hour == i & tweets.unique.arranged$Minute >= j & tweets.unique.arranged$Minute < j+5] = k\n#         k=k+1\n#         j=j+5\n#     }\n#     i=i+1\n#     j=0\n# }\n\n\n\nl = length(tweets.unique.arranged$text)\nn = floor(l / 100)\n\ntweets.unique.arranged$Time_bin = 100\n\nk = seq(0,l,n)\n\ni=1\n\nwhile (i<length(k)) {\n\n    tweets.unique.arranged$Time_bin[(k[i]:k[i+1])] = i\n    i=i+1\n\n    }\n\nplot(tweets.unique.arranged$Time_bin)\n\n#qplot(tweets.unique$lon, tweets.unique$lat, col = tweets.unique$Time_bin)\n\n\n#table(is.na(tweets.unique$lat))\n#table(is.na(tweets.unique$place_lat))\n\n\n#save(tweets.unique, file = 'tweets_all.RData')\nwrite.csv(tweets.unique, file = 'tweets_all.csv')\n\n# \n# \n# tweets.unique$Time = sapply(strsplit(tweets.unique$created_at, \" \"), \"[[\", 4)\n# tweets.unique$Time = as.POSIXlt(tweets.unique$Time, format = \"%H:%M:%S\")\n# tweets.unique$Time = tweets.unique$Time - 18000\n# \n# \n# hist(tweets.unique$Time, breaks = 50)\n# \n# ex = grepl('good morning', tweets.unique$text, ignore.case = TRUE)\n# tweets.good = tweets.unique[ex,]\n# \n# ex = grepl('Thu', tweets.unique$created_at, ignore.case = TRUE)\n# tweets.good = tweets.unique[ex,]\n# \n# hist(tweets.good$Time, breaks = 50)\n\n\n\n\n# Animation plotting ------------------------------------------------------\n\n\n#Load libraries,\nrequire(maptools)\nrequire(animation) \n\n#Get a world map,\ndata(wrld_simpl)\n\npar(mar = c(0,0,1,0),\n    pin = c(4,2),\n    pty = \"m\",\n    xaxs = \"i\",\n    xaxt = 'n',\n    xpd = FALSE,\n    yaxs = \"i\",\n    yaxt = 'n')\n\n\n#plot(tweets.unique$Time_bin)\n\n\ni = 1\n\n#Loop through the rows and save the gif...\n\nsaveGIF(while (i <= length(table(tweets.unique.arranged$Time_bin))) {\n    \n    plot(wrld_simpl, col='dark green', bg='white', border='black', ann=FALSE, axes = FALSE, main = \"Good morning, Twitter!\")\n    \n    points(tweets.unique.arranged$place_lon[tweets.unique.arranged$Time_bin == i], tweets.unique.arranged$place_lat[tweets.unique.arranged$Time_bin == i], pch = 16, col = 'red')\n\n    #abline(v = mean(tweets.unique.arranged$place_lon[tweets.unique.arranged$Time_bin == i]), col = 'red')\n    \n     #Plot some text,\n#      text(-125,110, paste(\"Time = \", mean(tweets.unique.arranged$Hour[tweets.unique.arranged$Time_bin == i]), \" hrs \",\n#                           floor(mean(tweets.unique.arranged$Minute[tweets.unique.arranged$Time_bin == i])), \" mins\", sep=\"\"),\n#                           col = \"black\", cex = 0.9, font = 2)\n    \n    temp = tweets.unique.arranged$DateTime[tweets.unique.arranged$Time_bin == i][1]    \n    \n    text(-40,110, paste(\"Date/Time = \", temp, \" GMT\"), col = \"black\", cex = 1.2, font = 2)\n    \n#     text(125,110, \"Colour = Day's weather\", col = \"black\", cex = 0.9, font = 2)\n#     \n#     points(85, 100, pch = 21, cex = 2, col = \"black\", bg = \"grey\")\n#     text(100, 101, \" - cloud \", col = \"black\", cex = 0.8, font = 2)\n#     \n#     points(115, 100, pch = 21, cex = 2, col = \"black\", bg = \"blue\")\n#     text(129, 101, \" - rain \", col = \"black\", cex = 0.8, font = 2)\n#     \n#     points(85, 92, pch = 21, cex = 2, col = \"black\", bg = \"light blue\")\n#     text(98, 93, \" - fine \", col = \"black\", cex = 0.8, font = 2)\n#     \n#     points(115, 92, pch = 21, cex = 2, col = \"black\")\n#     text(142, 93, \" - not recorded \", col = \"black\", cex = 0.8, font = 2)\n#     \n#     #text(-110,103, \"Ship's log: \", col = \"black\", cex = 1, font = 2)\n#     text(0, 110, paste(\"Date: \", Endeavour$Day[i],\"/\",Endeavour$Month[i],\"/\",Endeavour$Year[i]), col = \"black\", cex = 1, font = 2)\n#     #text(0, 100, Endeavour$Clearness[i], col = \"red\", cex = 1, font = 2)\n#     #text(0, 90, Endeavour$AllWindForces[i], col = \"red\", cex = 1, font = 2)\n#     #text(0, 90, Endeavour$PrecipitationDescriptor[i], col = \"red\", cex = 1, font = 2)\n#     \n#     #Wait a while between plots,\n#     ani.pause()\n#     \n#     #Wipe the text,\n#     #rect(-160,85,160,119, col = \"light blue\", border = NA)\n    \n    i = i+1\n    \n}, movie.name = \"goodmorning.gif\", img.name = \"Rplot\", interval = 0.1, convert = \"convert\", ani.width = 800, \nani.height = 800)\n\n\n\nqplot(tweets.unique.arranged$place_lon, tweets.unique.arranged$place_lat, col = as.factor(tweets.unique.arranged$Time_bin))\n\n\n\n\nmap <- NULL\nmapWorld <- borders(\"world\", colour=\"black\", fill=\"gray\")\nmap <- ggplot() +   mapWorld\n\n#Now Layer the cities on top\nmap <- map + geom_point(aes(x=tweets.unique.arranged$place_lon, y=tweets.unique.arranged$place_lat, \n                         color=as.factor(tweets.unique.arranged$Time_bin)) , size=3) + theme(legend.position=\"none\")\nmap\n\n\nplot(wrld_simpl, col='dark green', bg='white', border='black', ann=FALSE, axes = FALSE, main = \"Good morning, Twitter!\")\n\npoints(tweets.unique.arranged$place_lon, tweets.unique.arranged$place_lat, pch = 16, col = as.factor(tweets.unique.arranged$Time_bin))\n\n\n\n\nlibrary(ggplot2)\nlibrary(grid)\nfilterStream(\"tweetsUS.json\", locations = c(-125, 25, -66, 50), timeout = 300, \n             oauth = my_oauth)\ntweets.df <- parseTweets(\"tweetsUS.json\", verbose = FALSE)\nmap.data <- map_data(\"state\")\npoints <- data.frame(x = as.numeric(tweets.df$lon), y = as.numeric(tweets.df$lat))\npoints <- points[points$y > 25, ]\nggplot(map.data) + geom_map(aes(map_id = region), map = map.data, fill = \"white\", \n                            color = \"grey20\", size = 0.25) + expand_limits(x = map.data$long, y = map.data$lat) + \n    theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), \n          axis.title = element_blank(), panel.background = element_blank(), panel.border = element_blank(), \n          panel.grid.major = element_blank(), plot.background = element_blank(), \n          plot.margin = unit(0 * c(-1.5, -1.5, -1.5, -1.5), \"lines\")) + geom_point(data = points, \n                                                                                   aes(x = x, y = y), size = 1, alpha = 1/5, color = \"darkblue\")\n\n\n\ntable(tweets.df$country)\n\n\n\n\n\n# General Twitter searching -----------------------------------------------\n\n\n# Set API Keys\napi_key <- \"WgI0Z0p5feozqtasJdyKKG0ow\"\napi_secret <- \"8J87c6d5yxwig86yHdT8E5hX8GcBKWuLRzCpabNpAsYojXoOWE\"\naccess_token <- \"23829577-ma8Jk6S5GogQCvYOiQppdIYNSHdMKknakHh22JY1c\"\naccess_token_secret <- \"hhi7um7oev6QSKRxGmzl2TjKca8zYrfGH67YiBEy4rOEr\"\nmy_oauth = setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n\n\n\n# Grab latest tweets\ntweets_harris <- userTimeline('SamHarrisOrg', n = 100, includeRts = F)\ntweets_greenwald <- userTimeline('@ggreenwald', n = 1000, includeRts = T)\ntweets_greenwald = twListToDF(tweets_greenwald)\n\ntweets_article <- searchTwitter('A liberal writes in @Guardian how he was radicalized by @SamHarrisOrg', n=1000)\ntweets_article = twListToDF(tweets_article)\n\n?searchTwitter\n\nggplot(tweets_article, aes(x=created)) +\n    geom_histogram() +\n    theme_set(theme_bw(base_size = 18)) +\n    xlab(\"Date/Time created\") + \n    ggtitle(\"Histogram of Tweets for the 'A liberal writes...' article\")\n\n\n\n#tweets_harris = tweets_harris[tweets_harris$created > '2016-10-19 00:00:00',]\n\n\ntweets_SamHarrisOrg <- searchTwitter('from:SamHarrisOrg', n=100)\ntweets_SamHarrisOrg = twListToDF(tweets_SamHarrisOrg)\n\ntweets_greenwald = tweets_SamHarrisOrg[tweets_SamHarrisOrg$screenName == 'ggreenwald',]\n\n\ntweets_SamHarrisOrg_all <- searchTwitter('SamHarrisOrg', n=3000)\ntweets_SamHarrisOrg_all = twListToDF(tweets_SamHarrisOrg_all)\ntweets_greenwald_about_harris = tweets_SamHarrisOrg_all[tweets_SamHarrisOrg_all$screenName == 'ggreenwald',]\n\nrange(tweets_harris$created)\nrange(tweets_greenwald$created)\n\ntweets_harris = twListToDF(tweets_harris)\n\n\n# Loop over tweets and extract text\n#library(plyr)\n\n#feed_harris = laply(tweets_harris, function(t) t$getText())\n#feed_greenwald = laply(tweets_greenwald, function(t) t$getText())\n\nharris_mentions_greenwald = length(grep('@ggreenwald', tweets_harris$text))\ngreenwald_mentions_harris = length(grep('@SamHarrisOrg', tweets_greenwald$text))\n\nharris_mentions_islam = length(grep('islam', tweets_harris$text))\n\nplot(tweets_harris$created[grep('@ggreenwald', tweets_harris$text)], rownames(tweets_harris)[grep('@ggreenwald', tweets_harris$text)])\nplot(tweets_greenwald$created[grep('@SamHarrisOrg', tweets_greenwald$text)], rownames(tweets_greenwald)[grep('@SamHarrisOrg', tweets_greenwald$text)])\n\nplot(tweets_harris$created[grep('islam', tweets_harris$text)], rownames(tweets_harris)[grep('islam', tweets_harris$text)])\n\nplot(tweets_greenwald_all$created, rownames(tweets_greenwald_all), type = 'l')\nlines(tweets_SamHarrisOrg$created, rownames(tweets_SamHarrisOrg), col = 'red')\n\n\n\n\n\n\n\ndoInstall <- TRUE  # Change to FALSE if you don't want packages installed.\ntoInstall <- c(\"ROAuth\", \"igraph\", \"ggplot2\", \"wordcloud\", \"devtools\", \"tm\",\n               \"R2WinBUGS\", \"rmongodb\", \"scales\")\nif(doInstall){\n    install.packages(toInstall, repos = \"http://cran.r-project.org\")\n    library(devtools)\n    # R packages to get twitter and Facebook data\n    install_github(\"streamR\", \"pablobarbera\", subdir=\"streamR\")\n    install_github(\"Rfacebook\", \"pablobarbera\", subdir=\"Rfacebook\")\n    # smapp R package\n    install_github(\"smappR\", \"SMAPPNYU\")\n}",
    "created" : 1495827896972.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1625295211",
    "id" : "6035E0E6",
    "lastKnownWriteTime" : 1496050848,
    "last_content_update" : 1496050848,
    "path" : "~/Dropbox/Kaggle/JihadWatch/twitter.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}