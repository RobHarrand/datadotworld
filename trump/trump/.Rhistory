library('tm')
setwd('/home/rob/Dropbox/Kaggle/JihadWatch')
load(file = "my_oauth.Rdata") #Authenication data
load(file = 'tweets.RData') #LOAD THIS AS IT'S THE TWEET TEMPLATE
noTs_history = read.csv('noTs.csv')
i = 1
Sys.time()
#while (i <= 280) {
while (i <= 1) {
filterStream("tw_t.json",
timeout = 10,
oauth = my_oauth,
track = c('taghut',
'mushrikeen',
'rafidah',
'kufar',
'kuffir',
'kuffar',
'jizya',
'khilafa'),
language = 'en')
#filterStream("tw_t.json", timeout = 30, oauth = my_oauth, track = c('dog', 'cat'), language = 'en')
#filterStream("tw_t.json", timeout = 10, oauth = my_oauth, track = 'hello', language = 'en')
tweets_t = parseTweets("tw_t.json")
#ex = tweets_gm$lang != 'ar'
#tweets_gm = tweets_gm[!ex,]
#ex = grepl('RT', tweets_gm$text, ignore.case = FALSE) #Remove the RTs
#tweets_gm = tweets_gm[!ex,]
#ex = grepl('taghut|mushrikeen|rafidah|kufar', tweets_gm$text, ignore.case = TRUE) #Remove anything without good morning in the main tweet text
#tweets_gm = tweets_gm[ex,]
#ex = is.na(tweets_gm$place_lat) #Remove any with missing place_latitude information
#tweets_gm = tweets_gm[!ex,]
#tweets.all = rbind(tweets.all, tweets_t) #Add to the collection
#
# res <- translate(content.vec = "Hello world",
#                  microsoft.client.id = "dc10d31f-4c1f-4e27-8928-8bc85bfd0e6d",
#                  microsoft.client.secret = "790dc39f94ba40ea86cc11eb71aa2f2a",
#                  source.lang = "en",
#                  target.lang = "de")
# res
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
tweets_t$text = removeURL(tweets_t$text)
removeStuff <- function(x) gsub("[[:punct:]]", " ", x)
tweets_t$text = removeStuff(tweets_t$text)
noTs = length(tweets_t$text)
noRTs = table(grepl('RT', tweets_t$text, ignore.case = FALSE))[2]
tweets_t$text <- iconv(tweets_t$text, 'UTF-8', 'ASCII')
corpus = Corpus(VectorSource(list(tweets_t$text)))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
#corpus = tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
corpus = tm_map(corpus, removeWords, stopwords('english'))
dtm_tweets = DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
freq_tweets <- colSums(as.matrix(dtm_tweets))
#sentiments = calculate_sentiment(names(freq_tweets))
#sentiments = cbind(sentiments, as.data.frame(freq_tweets))
freq = as.data.frame(freq_tweets)
keywords = c('taghut', 'mushrikeen', 'rafidah', 'kufar', 'kuffir', 'kuffar', 'jizya', 'khilafa')
freq$Words = row.names(freq)
ex = freq$Words %in% keywords
freq = freq[!ex,]
noTs_new = data.frame('Date' = Sys.time(), 'noTs' = noTs, 'noRTs' = noRTs)
noTs_history = rbind(noTs_history, noTs_new)
date_for_file = gsub(' ', '_', noTs_new$Date)
date_for_file = gsub(':', '_', date_for_file)
}
i = 1
Sys.time()
#while (i <= 280) {
while (i <= 1) {
filterStream("tw_t.json",
timeout = 10,
oauth = my_oauth,
track = c('taghut',
'mushrikeen',
'rafidah',
'kufar',
'kuffir',
'kuffar',
'jizya',
'khilafa', 'hello'),
language = 'en')
#filterStream("tw_t.json", timeout = 30, oauth = my_oauth, track = c('dog', 'cat'), language = 'en')
#filterStream("tw_t.json", timeout = 10, oauth = my_oauth, track = 'hello', language = 'en')
tweets_t = parseTweets("tw_t.json")
#ex = tweets_gm$lang != 'ar'
#tweets_gm = tweets_gm[!ex,]
#ex = grepl('RT', tweets_gm$text, ignore.case = FALSE) #Remove the RTs
#tweets_gm = tweets_gm[!ex,]
#ex = grepl('taghut|mushrikeen|rafidah|kufar', tweets_gm$text, ignore.case = TRUE) #Remove anything without good morning in the main tweet text
#tweets_gm = tweets_gm[ex,]
#ex = is.na(tweets_gm$place_lat) #Remove any with missing place_latitude information
#tweets_gm = tweets_gm[!ex,]
#tweets.all = rbind(tweets.all, tweets_t) #Add to the collection
#
# res <- translate(content.vec = "Hello world",
#                  microsoft.client.id = "dc10d31f-4c1f-4e27-8928-8bc85bfd0e6d",
#                  microsoft.client.secret = "790dc39f94ba40ea86cc11eb71aa2f2a",
#                  source.lang = "en",
#                  target.lang = "de")
# res
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
tweets_t$text = removeURL(tweets_t$text)
removeStuff <- function(x) gsub("[[:punct:]]", " ", x)
tweets_t$text = removeStuff(tweets_t$text)
noTs = length(tweets_t$text)
noRTs = table(grepl('RT', tweets_t$text, ignore.case = FALSE))[2]
tweets_t$text <- iconv(tweets_t$text, 'UTF-8', 'ASCII')
corpus = Corpus(VectorSource(list(tweets_t$text)))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
#corpus = tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
corpus = tm_map(corpus, removeWords, stopwords('english'))
dtm_tweets = DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
freq_tweets <- colSums(as.matrix(dtm_tweets))
#sentiments = calculate_sentiment(names(freq_tweets))
#sentiments = cbind(sentiments, as.data.frame(freq_tweets))
freq = as.data.frame(freq_tweets)
keywords = c('taghut', 'mushrikeen', 'rafidah', 'kufar', 'kuffir', 'kuffar', 'jizya', 'khilafa')
freq$Words = row.names(freq)
ex = freq$Words %in% keywords
freq = freq[!ex,]
noTs_new = data.frame('Date' = Sys.time(), 'noTs' = noTs, 'noRTs' = noRTs)
noTs_history = rbind(noTs_history, noTs_new)
date_for_file = gsub(' ', '_', noTs_new$Date)
date_for_file = gsub(':', '_', date_for_file)
}
jpeg(paste('t', res = 800, date_for_file, '.jpg', sep = ""))
par(mfrow=c(2,1))
par(mar=c(4,4,1,4))
set.seed(100)
wordcloud(row.names(freq),
freq$freq_tweets,
min.freq=max(freq$freq_tweets)*0.3,
colors=brewer.pal(6,"Dark2"),
random.order = T,
random.color = T,
rot.per = 0.3,
scale = c(1.5, 0.1))
plot(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
xlim = c(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S"),
as.POSIXct(noTs_history$Date[length(noTs_history$Date)], format="%Y-%m-%d %H:%M:%S")),
ylim = c(0, max(noTs_history$noTs)*1.2),
ylab = 'No. of tweets',
xlab = 'Date',
type = 'l',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
pch = 16)
lines(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
col = 'red',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
pch = 16,
col = 'red')
legend(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S")+100,
max(noTs_history$noTs)*1.1,
lty=c(1,1),
lwd=c(2.5,2.5),
legend =c('Tweets', 'Re-tweets'),
col=c('black', 'red'))
dev.off()
jpeg(paste('t', res = 800, width = 500, height = 1000, date_for_file, '.jpg', sep = ""))
par(mfrow=c(2,1))
par(mar=c(4,4,1,4))
set.seed(100)
wordcloud(row.names(freq),
freq$freq_tweets,
min.freq=max(freq$freq_tweets)*0.3,
colors=brewer.pal(6,"Dark2"),
random.order = T,
random.color = T,
rot.per = 0.3,
scale = c(1.5, 0.1))
plot(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
xlim = c(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S"),
as.POSIXct(noTs_history$Date[length(noTs_history$Date)], format="%Y-%m-%d %H:%M:%S")),
ylim = c(0, max(noTs_history$noTs)*1.2),
ylab = 'No. of tweets',
xlab = 'Date',
type = 'l',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
pch = 16)
lines(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
col = 'red',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
pch = 16,
col = 'red')
legend(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S")+100,
max(noTs_history$noTs)*1.1,
lty=c(1,1),
lwd=c(2.5,2.5),
legend =c('Tweets', 'Re-tweets'),
col=c('black', 'red'))
dev.off()
png(paste('t', res = 800, width = 500, height = 1000, date_for_file, '.png', sep = ""))
par(mfrow=c(2,1))
par(mar=c(4,4,1,4))
set.seed(100)
wordcloud(row.names(freq),
freq$freq_tweets,
min.freq=max(freq$freq_tweets)*0.3,
colors=brewer.pal(6,"Dark2"),
random.order = T,
random.color = T,
rot.per = 0.3,
scale = c(1.5, 0.1))
plot(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
xlim = c(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S"),
as.POSIXct(noTs_history$Date[length(noTs_history$Date)], format="%Y-%m-%d %H:%M:%S")),
ylim = c(0, max(noTs_history$noTs)*1.2),
ylab = 'No. of tweets',
xlab = 'Date',
type = 'l',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
pch = 16)
lines(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
col = 'red',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
pch = 16,
col = 'red')
legend(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S")+100,
max(noTs_history$noTs)*1.1,
lty=c(1,1),
lwd=c(2.5,2.5),
legend =c('Tweets', 'Re-tweets'),
col=c('black', 'red'))
dev.off()
set.seed(100)
wordcloud(row.names(freq),
freq$freq_tweets,
min.freq=max(freq$freq_tweets)*0.3,
colors=brewer.pal(6,"Dark2"),
random.order = T,
random.color = T,
rot.per = 0.3,
scale = c(1.5, 0.1))
plot(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
xlim = c(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S"),
as.POSIXct(noTs_history$Date[length(noTs_history$Date)], format="%Y-%m-%d %H:%M:%S")),
ylim = c(0, max(noTs_history$noTs)*1.2),
ylab = 'No. of tweets',
xlab = 'Date',
type = 'l',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
pch = 16)
lines(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
col = 'red',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
pch = 16,
col = 'red')
legend(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S")+100,
max(noTs_history$noTs)*1.1,
lty=c(1,1),
lwd=c(2.5,2.5),
legend =c('Tweets', 'Re-tweets'),
col=c('black', 'red'))
set.seed(100)
wordcloud(row.names(freq),
freq$freq_tweets,
min.freq=max(freq$freq_tweets)*0.3,
colors=brewer.pal(6,"Dark2"),
random.order = T,
random.color = T,
rot.per = 0.3,
scale = c(1.5, 0.1))
as.POSIXct(noTs_history$Date)
load(file = "my_oauth.Rdata") #Authenication data
load(file = 'tweets.RData') #LOAD THIS AS IT'S THE TWEET TEMPLATE
noTs_history = read.csv('noTs.csv')
i = 1
Sys.time()
#while (i <= 280) {
while (i <= 10000) {
filterStream("tw_t.json",
timeout = 10,
oauth = my_oauth,
track = c('taghut',
'mushrikeen',
'rafidah',
'kufar',
'kuffir',
'kuffar',
'jizya',
'khilafa'),
language = 'en')
#filterStream("tw_t.json", timeout = 30, oauth = my_oauth, track = c('dog', 'cat'), language = 'en')
#filterStream("tw_t.json", timeout = 10, oauth = my_oauth, track = 'hello', language = 'en')
tweets_t = parseTweets("tw_t.json")
#ex = tweets_gm$lang != 'ar'
#tweets_gm = tweets_gm[!ex,]
#ex = grepl('RT', tweets_gm$text, ignore.case = FALSE) #Remove the RTs
#tweets_gm = tweets_gm[!ex,]
#ex = grepl('taghut|mushrikeen|rafidah|kufar', tweets_gm$text, ignore.case = TRUE) #Remove anything without good morning in the main tweet text
#tweets_gm = tweets_gm[ex,]
#ex = is.na(tweets_gm$place_lat) #Remove any with missing place_latitude information
#tweets_gm = tweets_gm[!ex,]
#tweets.all = rbind(tweets.all, tweets_t) #Add to the collection
#
# res <- translate(content.vec = "Hello world",
#                  microsoft.client.id = "dc10d31f-4c1f-4e27-8928-8bc85bfd0e6d",
#                  microsoft.client.secret = "790dc39f94ba40ea86cc11eb71aa2f2a",
#                  source.lang = "en",
#                  target.lang = "de")
# res
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
tweets_t$text = removeURL(tweets_t$text)
removeStuff <- function(x) gsub("[[:punct:]]", " ", x)
tweets_t$text = removeStuff(tweets_t$text)
noTs = length(tweets_t$text)
noRTs = table(grepl('RT', tweets_t$text, ignore.case = FALSE))[2]
tweets_t$text <- iconv(tweets_t$text, 'UTF-8', 'ASCII')
corpus = Corpus(VectorSource(list(tweets_t$text)))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
#corpus = tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
corpus = tm_map(corpus, removeWords, stopwords('english'))
dtm_tweets = DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
freq_tweets <- colSums(as.matrix(dtm_tweets))
#sentiments = calculate_sentiment(names(freq_tweets))
#sentiments = cbind(sentiments, as.data.frame(freq_tweets))
freq = as.data.frame(freq_tweets)
keywords = c('taghut', 'mushrikeen', 'rafidah', 'kufar', 'kuffir', 'kuffar', 'jizya', 'khilafa')
freq$Words = row.names(freq)
ex = freq$Words %in% keywords
freq = freq[!ex,]
noTs_new = data.frame('Date' = Sys.time(), 'noTs' = noTs, 'noRTs' = noRTs)
noTs_history = rbind(noTs_history, noTs_new)
date_for_file = gsub(' ', '_', noTs_new$Date)
date_for_file = gsub(':', '_', date_for_file)
png(paste('t', res = 800, width = 500, height = 1000, date_for_file, '.png', sep = ""))
par(mfrow=c(2,1))
par(mar=c(4,4,1,4))
set.seed(100)
wordcloud(row.names(freq),
freq$freq_tweets,
min.freq=max(freq$freq_tweets)*0.3,
colors=brewer.pal(6,"Dark2"),
random.order = T,
random.color = T,
rot.per = 0.3,
scale = c(1.5, 0.1))
plot(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
xlim = c(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S"),
as.POSIXct(noTs_history$Date[length(noTs_history$Date)], format="%Y-%m-%d %H:%M:%S")),
ylim = c(0, max(noTs_history$noTs)*1.2),
ylab = 'No. of tweets',
xlab = 'Date',
type = 'l',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
pch = 16)
lines(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
col = 'red',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
pch = 16,
col = 'red')
legend(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S")+100,
max(noTs_history$noTs)*1.1,
lty=c(1,1),
lwd=c(2.5,2.5),
legend =c('Tweets', 'Re-tweets'),
col=c('black', 'red'))
dev.off()
i=i+1
tweet(paste("Update at", Sys.time(), '#radicalism'),
mediaPath = paste('t', date_for_file, '.png', sep = ""))
fn <- "tw_t.json"
if (file.exists(fn)) file.remove(fn)
#tweet('test')
}
i = 1
Sys.time()
#while (i <= 280) {
while (i <= 10000) {
filterStream("tw_t.json",
timeout = 3600,
oauth = my_oauth,
track = c('taghut',
'mushrikeen',
'rafidah',
'kufar',
'kuffir',
'kuffar',
'jizya',
'khilafa'),
language = 'en')
#filterStream("tw_t.json", timeout = 30, oauth = my_oauth, track = c('dog', 'cat'), language = 'en')
#filterStream("tw_t.json", timeout = 10, oauth = my_oauth, track = 'hello', language = 'en')
tweets_t = parseTweets("tw_t.json")
#ex = tweets_gm$lang != 'ar'
#tweets_gm = tweets_gm[!ex,]
#ex = grepl('RT', tweets_gm$text, ignore.case = FALSE) #Remove the RTs
#tweets_gm = tweets_gm[!ex,]
#ex = grepl('taghut|mushrikeen|rafidah|kufar', tweets_gm$text, ignore.case = TRUE) #Remove anything without good morning in the main tweet text
#tweets_gm = tweets_gm[ex,]
#ex = is.na(tweets_gm$place_lat) #Remove any with missing place_latitude information
#tweets_gm = tweets_gm[!ex,]
#tweets.all = rbind(tweets.all, tweets_t) #Add to the collection
#
# res <- translate(content.vec = "Hello world",
#                  microsoft.client.id = "dc10d31f-4c1f-4e27-8928-8bc85bfd0e6d",
#                  microsoft.client.secret = "790dc39f94ba40ea86cc11eb71aa2f2a",
#                  source.lang = "en",
#                  target.lang = "de")
# res
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
tweets_t$text = removeURL(tweets_t$text)
removeStuff <- function(x) gsub("[[:punct:]]", " ", x)
tweets_t$text = removeStuff(tweets_t$text)
noTs = length(tweets_t$text)
noRTs = table(grepl('RT', tweets_t$text, ignore.case = FALSE))[2]
tweets_t$text <- iconv(tweets_t$text, 'UTF-8', 'ASCII')
corpus = Corpus(VectorSource(list(tweets_t$text)))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
#corpus = tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
corpus = tm_map(corpus, removeWords, stopwords('english'))
dtm_tweets = DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
freq_tweets <- colSums(as.matrix(dtm_tweets))
#sentiments = calculate_sentiment(names(freq_tweets))
#sentiments = cbind(sentiments, as.data.frame(freq_tweets))
freq = as.data.frame(freq_tweets)
keywords = c('taghut', 'mushrikeen', 'rafidah', 'kufar', 'kuffir', 'kuffar', 'jizya', 'khilafa')
freq$Words = row.names(freq)
ex = freq$Words %in% keywords
freq = freq[!ex,]
noTs_new = data.frame('Date' = Sys.time(), 'noTs' = noTs, 'noRTs' = noRTs)
noTs_history = rbind(noTs_history, noTs_new)
date_for_file = gsub(' ', '_', noTs_new$Date)
date_for_file = gsub(':', '_', date_for_file)
png(paste('t', res = 800, width = 500, height = 1000, date_for_file, '.png', sep = ""))
par(mfrow=c(2,1))
par(mar=c(4,4,1,4))
set.seed(100)
wordcloud(row.names(freq),
freq$freq_tweets,
min.freq=max(freq$freq_tweets)*0.3,
colors=brewer.pal(6,"Dark2"),
random.order = T,
random.color = T,
rot.per = 0.3,
scale = c(1.5, 0.1))
plot(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
xlim = c(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S"),
as.POSIXct(noTs_history$Date[length(noTs_history$Date)], format="%Y-%m-%d %H:%M:%S")),
ylim = c(0, max(noTs_history$noTs)*1.2),
ylab = 'No. of tweets',
xlab = 'Date',
type = 'l',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noTs,
pch = 16)
lines(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
col = 'red',
lwd = 2)
points(as.POSIXct(noTs_history$Date),
noTs_history$noRTs,
pch = 16,
col = 'red')
legend(as.POSIXct(noTs_history$Date[1], format="%Y-%m-%d %H:%M:%S")+100,
max(noTs_history$noTs)*1.1,
lty=c(1,1),
lwd=c(2.5,2.5),
legend =c('Tweets', 'Re-tweets'),
col=c('black', 'red'))
dev.off()
i=i+1
tweet(paste("Update at", Sys.time(), '#radicalism'),
mediaPath = paste('t', date_for_file, '.png', sep = ""))
fn <- "tw_t.json"
if (file.exists(fn)) file.remove(fn)
#tweet('test')
}
